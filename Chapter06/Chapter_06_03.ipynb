{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN on DLVM (GPU)\n",
    "In this notebook we will go through the steps of stacking layers together and seeing how it affects performance. In section we went through the steps of how the various types of layers and their properties affect the dimensions of the data passing through them. In this notebook we will look at the affect on performance so that we get an idea of stacking these layers can give us better performance. We will be basic this CNN on the VGG architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import pickle\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EPOCHS = 10\n",
    "BATCHSIZE = 64\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "N_CLASSES = 10 # There are 10 classes in the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format = 'channels_first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(src):\n",
    "    with open(src, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin1')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cifar():\n",
    "    \"\"\" Read data\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Preparing train set...')\n",
    "    train_list = [read_pickle('./cifar-10-batches-py/data_batch_{0}'.format(i)) for i in range(1, 6)]\n",
    "    x_train = np.concatenate([t['data'] for t in train_list])\n",
    "    y_train = np.concatenate([t['labels'] for t in train_list])\n",
    "    \n",
    "    print('Preparing test set...')\n",
    "    tst = read_pickle('./cifar-10-batches-py/test_batch')\n",
    "    x_test = tst['data']\n",
    "    y_test = np.asarray(tst['labels'])\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(src=\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"):\n",
    "    \"\"\" Load CIFAR10 Dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return process_cifar()\n",
    "    except FileNotFoundError:\n",
    "        print('Data does not exist. Downloading ' + src)\n",
    "        fname, h = urlretrieve(src, './delete.me')\n",
    "        print('Extracting files...')\n",
    "        with tarfile.open(fname) as tar:\n",
    "            tar.extractall()\n",
    "        os.remove(fname)\n",
    "    return process_cifar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cifar(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # Scale pixel intensity\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "    \n",
    "    # Reshape\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32)\n",
    "    x_test = x_test.reshape(-1, 3, 32, 32)\n",
    "    \n",
    "    return (x_train.astype(np.float32), \n",
    "            y_train.astype(np.int32), \n",
    "            x_test.astype(np.float32), \n",
    "            y_test.astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_from(X, y, batchsize=BATCHSIZE, shuffle=False):\n",
    "    if len(X) != len(y):\n",
    "        raise Exception(\"The length of X {} and y {} don't match\".format(len(X), len(y)))\n",
    "        \n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y)\n",
    "    \n",
    "    for i in range(0, len(X), batchsize):\n",
    "        yield X[i:i + batchsize], y[i:i + batchsize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):\n",
    "    index = np.arange(len(X))\n",
    "    np.random.shuffle(index)\n",
    "    return X[index], y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_training(m, labels, learning_rate=LR, momentum=MOMENTUM):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=m, labels=y)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum)\n",
    "    return optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Data into format for library\n",
    "x_train, y_train, x_test, y_test = prepare_cifar(*load_cifar())\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print(x_train.dtype, x_test.dtype, y_train.dtype, y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "Our first model will have 2 convolution layers and a max pooling layer. The classification layer will use softmax as we want it to only output a 1 for our specified class and 0 everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Place-holders\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3, 32, 32])\n",
    "y = tf.placeholder(tf.int32, shape=[None])\n",
    "training = tf.placeholder(tf.bool)  # Indicator for dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1\n",
    "conv1_1 = tf.layers.conv2d(X, \n",
    "                           filters=64, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "conv1_2 = tf.layers.conv2d(conv1_1, \n",
    "                           filters=64, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "pool1_1 = tf.layers.max_pooling2d(conv1_2, \n",
    "                                  pool_size=(2, 2), \n",
    "                                  strides=(2, 2), \n",
    "                                  padding='valid', \n",
    "                                  data_format=data_format)\n",
    "relu2 = tf.nn.relu(pool1_1)\n",
    "flatten = tf.reshape(relu2, shape=[-1, 64*16*16])\n",
    "fc1 = tf.layers.dense(flatten, 4096, activation=tf.nn.relu)\n",
    "fc2 = tf.layers.dense(fc1, 4096, activation=tf.nn.relu)\n",
    "model = tf.layers.dense(fc2, N_CLASSES, name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model = init_model_training(model, y)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Accuracy logging\n",
    "correct = tf.nn.in_top_k(model, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train model\n",
    "for j in range(EPOCHS):\n",
    "    for data, label in minibatch_from(x_train, y_train, shuffle=True):\n",
    "        sess.run(train_model, feed_dict={X: data, y: label})\n",
    "    # Log\n",
    "    acc_train = sess.run(accuracy, feed_dict={X: data, y: label})\n",
    "    print(j, \"Train accuracy:\", acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_guess = list()\n",
    "for data, label in minibatch_from(x_test, y_test):\n",
    "    pred = tf.argmax(model,1)\n",
    "    output = sess.run(pred, feed_dict={X: data})\n",
    "    y_guess.append(output)\n",
    "print(\"Accuracy: \", sum(np.concatenate(y_guess) == y_test)/float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model gets an accuracy of around 68% on the test set after 5 epochs. We can also see that it achieves 100% on the training set a few epochs before we stop training. It would usually be prudent to stop the model earlier and there are usually callbacks that can be used in any of the frameworks to do this. We are simply not using these here to try and keep things simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 \n",
    "With the second model we will add a second convolution block. In keeping with the VGG architecture we will add two convolution layers each with 128 filters as well as a mac pooling layer. This time we will train it for 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "# Place-holders\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3, 32, 32])\n",
    "y = tf.placeholder(tf.int32, shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1\n",
    "conv1_1 = tf.layers.conv2d(X, \n",
    "                           filters=64, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "conv1_2 = tf.layers.conv2d(conv1_1, \n",
    "                           filters=64, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "pool1_1 = tf.layers.max_pooling2d(conv1_2, \n",
    "                                  pool_size=(2, 2), \n",
    "                                  strides=(2, 2), \n",
    "                                  padding='valid', \n",
    "                                  data_format=data_format)\n",
    "# Block 2\n",
    "conv2_1 = tf.layers.conv2d(pool1_1, \n",
    "                           filters=128, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "conv2_2 = tf.layers.conv2d(conv2_1, \n",
    "                           filters=128, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "pool2_1 = tf.layers.max_pooling2d(conv2_2, \n",
    "                                  pool_size=(2, 2), \n",
    "                                  strides=(2, 2), \n",
    "                                  padding='valid', \n",
    "                                  data_format=data_format)\n",
    "\n",
    "relu2 = tf.nn.relu(pool2_1)\n",
    "flatten = tf.reshape(relu2, shape=[-1, 128*8*8])\n",
    "fc1 = tf.layers.dense(flatten, 4096, activation=tf.nn.relu)\n",
    "fc2 = tf.layers.dense(fc1, 4096, activation=tf.nn.relu)\n",
    "model = tf.layers.dense(fc2, N_CLASSES, name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model = init_model_training(model, y)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Accuracy logging\n",
    "correct = tf.nn.in_top_k(model, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train model\n",
    "for j in range(EPOCHS):\n",
    "    for data, label in minibatch_from(x_train, y_train, shuffle=True):\n",
    "        sess.run(train_model, feed_dict={X: data, y: label})\n",
    "    # Log\n",
    "    acc_train = sess.run(accuracy, feed_dict={X: data, y: label})\n",
    "    print(j, \"Train accuracy:\", acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_guess = list()\n",
    "for data, label in minibatch_from(x_test, y_test):\n",
    "    pred = tf.argmax(model,1)\n",
    "    output = sess.run(pred, feed_dict={X: data})\n",
    "    y_guess.append(output)\n",
    "print(\"Accuracy: \", sum(np.concatenate(y_guess) == y_test)/float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does slightly better with an accuracy of 75.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 \n",
    "For our third model we will add a 3rd convolution block. This will be made up of 3 convolution layers each with 256 filters each. Again we will have a max pooling block at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "# Place-holders\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3, 32, 32])\n",
    "y = tf.placeholder(tf.int32, shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1\n",
    "conv1_1 = tf.layers.conv2d(X, \n",
    "                           filters=64, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "conv1_2 = tf.layers.conv2d(conv1_1, \n",
    "                           filters=64, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "pool1_1 = tf.layers.max_pooling2d(conv1_2, \n",
    "                                  pool_size=(2, 2), \n",
    "                                  strides=(2, 2), \n",
    "                                  padding='valid', \n",
    "                                  data_format=data_format)\n",
    "# Block 2\n",
    "conv2_1 = tf.layers.conv2d(pool1_1, \n",
    "                           filters=128, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "conv2_2 = tf.layers.conv2d(conv2_1, \n",
    "                           filters=128, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "pool2_1 = tf.layers.max_pooling2d(conv2_2, \n",
    "                                  pool_size=(2, 2), \n",
    "                                  strides=(2, 2), \n",
    "                                  padding='valid', \n",
    "                                  data_format=data_format)\n",
    "\n",
    "# Block 3\n",
    "conv3_1 = tf.layers.conv2d(pool2_1, \n",
    "                           filters=256, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "conv3_2 = tf.layers.conv2d(conv3_1, \n",
    "                           filters=256, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "conv3_3 = tf.layers.conv2d(conv3_2, \n",
    "                           filters=256, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "pool3_1 = tf.layers.max_pooling2d(conv3_3, \n",
    "                                  pool_size=(2, 2), \n",
    "                                  strides=(2, 2), \n",
    "                                  padding='valid', \n",
    "                                  data_format=data_format)\n",
    "\n",
    "relu2 = tf.nn.relu(pool3_1)\n",
    "flatten = tf.reshape(relu2, shape=[-1, 256*4*4])\n",
    "fc1 = tf.layers.dense(flatten, 4096, activation=tf.nn.relu)\n",
    "fc2 = tf.layers.dense(fc1, 4096, activation=tf.nn.relu)\n",
    "model = tf.layers.dense(fc2, N_CLASSES, name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model = init_model_training(model, y)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Accuracy logging\n",
    "correct = tf.nn.in_top_k(model, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train model\n",
    "for j in range(EPOCHS):\n",
    "    for data, label in minibatch_from(x_train, y_train, shuffle=True):\n",
    "        sess.run(train_model, feed_dict={X: data, y: label})\n",
    "    # Log\n",
    "    acc_train = sess.run(accuracy, feed_dict={X: data, y: label})\n",
    "    print(j, \"Train accuracy:\", acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_guess = list()\n",
    "for data, label in minibatch_from(x_test, y_test):\n",
    "    pred = tf.argmax(model,1)\n",
    "    output = sess.run(pred, feed_dict={X: data})\n",
    "    y_guess.append(output)\n",
    "print(\"Accuracy: \", sum(np.concatenate(y_guess) == y_test)/float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model reaches a accuracy of 76%. As you can see with each additional layer we get better results but the returns diminish with each succesive block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4\n",
    "Due to the large number of free parameters CNNs can benefit from regularisation. One way to refularise is to use a dropout layer which we talked about earlier. This layer will randomly during the forward pass zero a certain proportion of its outputs. This was also eployed by the authors of the VGG architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "# Place-holders\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3, 32, 32])\n",
    "y = tf.placeholder(tf.int32, shape=[None])\n",
    "training = tf.placeholder(tf.bool)  # Indicator for dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1\n",
    "conv1_1 = tf.layers.conv2d(X, \n",
    "                           filters=64, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "conv1_2 = tf.layers.conv2d(conv1_1, \n",
    "                           filters=64, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "pool1_1 = tf.layers.max_pooling2d(conv1_2, \n",
    "                                  pool_size=(2, 2), \n",
    "                                  strides=(2, 2), \n",
    "                                  padding='valid', \n",
    "                                  data_format=data_format)\n",
    "# Block 2\n",
    "conv2_1 = tf.layers.conv2d(pool1_1, \n",
    "                           filters=128, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "conv2_2 = tf.layers.conv2d(conv2_1, \n",
    "                           filters=128, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "pool2_1 = tf.layers.max_pooling2d(conv2_2, \n",
    "                                  pool_size=(2, 2), \n",
    "                                  strides=(2, 2), \n",
    "                                  padding='valid', \n",
    "                                  data_format=data_format)\n",
    "\n",
    "# Block 3\n",
    "conv3_1 = tf.layers.conv2d(pool2_1, \n",
    "                           filters=256, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "conv3_2 = tf.layers.conv2d(conv3_1, \n",
    "                           filters=256, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "conv3_3 = tf.layers.conv2d(conv3_2, \n",
    "                           filters=256, \n",
    "                           kernel_size=(3, 3), \n",
    "                           padding='same', \n",
    "                           data_format=data_format,\n",
    "                           activation=tf.nn.relu)\n",
    "pool3_1 = tf.layers.max_pooling2d(conv3_3, \n",
    "                                  pool_size=(2, 2), \n",
    "                                  strides=(2, 2), \n",
    "                                  padding='valid', \n",
    "                                  data_format=data_format)\n",
    "\n",
    "relu2 = tf.nn.relu(pool3_1)\n",
    "flatten = tf.reshape(relu2, shape=[-1, 256*4*4])\n",
    "fc1 = tf.layers.dense(flatten, 4096, activation=tf.nn.relu)\n",
    "drop1 = tf.layers.dropout(fc1, 0.5, training=training)\n",
    "fc2 = tf.layers.dense(drop1, 4096, activation=tf.nn.relu)\n",
    "drop2 = tf.layers.dropout(fc2, 0.5, training=training)\n",
    "model = tf.layers.dense(drop2, N_CLASSES, name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model = init_model_training(model, y)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Accuracy logging\n",
    "correct = tf.nn.in_top_k(model, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train model\n",
    "for j in range(EPOCHS):\n",
    "    for data, label in minibatch_from(x_train, y_train, shuffle=True):\n",
    "        sess.run(train_model, feed_dict={X: data, y: label, training: True})\n",
    "    # Log\n",
    "    acc_train = sess.run(accuracy, feed_dict={X: data, y: label, training: True})\n",
    "    print(j, \"Train accuracy:\", acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_guess = list()\n",
    "for data, label in minibatch_from(x_test, y_test):\n",
    "    pred = tf.argmax(model,1)\n",
    "    output = sess.run(pred, feed_dict={X: data, training: False})\n",
    "    y_guess.append(output)\n",
    "print(\"Accuracy: \", sum(np.concatenate(y_guess) == y_test)/float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our accuracy has increased further to 80%. The VGG architecture actually has even more layers than our final model but it was designed to tackle the ImageNet dataset which contains a lot more data than the CIFAR10 dataset. Adding further layers with the limited data available would quickly prove untenable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
