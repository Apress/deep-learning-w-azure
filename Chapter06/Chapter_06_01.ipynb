{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN on CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "Here we define the parameters for our training. We will be using simple stochastic gradient descent with momentum. We are using standard values for the learning rate and momentum. We have set the number of epochs to 3 since this is designed to run on Azure Notebooks with limited computational resources. If you run this on your own machine or a VM with a GPU you can increase this to 10 or more so you get a better idea of how the model trains. The batch size is the number of examples to group together in a batch. The size of your batch may be limited by the memory available on your GPU if you run this notebook on a GPU enabled machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EPOCHS = 3\n",
    "BATCHSIZE = 64\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "N_CLASSES = 10 # There are 10 classes in the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for downloading and preparing the data\n",
    "The CIFAR10 dataset comes as a number of pickle files. 5 for training and 1 for testing. These pickle files contain the data as numpy arrays which is convenient since we don't have to transform the images into numpy arrays. Each row of the dataset is a 3x32x32 image. The dimensions represents the Height, Width and Color channels of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(src):\n",
    "    with open(src, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin1')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cifar():\n",
    "    \"\"\" Read data\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Preparing train set...')\n",
    "    train_list = [read_pickle('./cifar-10-batches-py/data_batch_{0}'.format(i)) for i in range(1, 6)]\n",
    "    x_train = np.concatenate([t['data'] for t in train_list])\n",
    "    y_train = np.concatenate([t['labels'] for t in train_list])\n",
    "    \n",
    "    print('Preparing test set...')\n",
    "    tst = read_pickle('./cifar-10-batches-py/test_batch')\n",
    "    x_test = tst['data']\n",
    "    y_test = np.asarray(tst['labels'])\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(src=\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"):\n",
    "    \"\"\" Load CIFAR10 Dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return process_cifar()\n",
    "    except FileNotFoundError:\n",
    "        print('Data does not exist. Downloading ' + src)\n",
    "        fname, h = urlretrieve(src, './delete.me')\n",
    "        print('Extracting files...')\n",
    "        with tarfile.open(fname) as tar:\n",
    "            tar.extractall()\n",
    "        os.remove(fname)\n",
    "    return process_cifar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to use the data we have to do some preprocessing.\n",
    "* First we need to scale the pixel value to be between 0 and 1\n",
    "* Then we need to reshape the vector to be a 3 dimensional vector\n",
    "* Then convert the data to the appropriate data type so that it is treated properly by the following functions but also to reduce memory overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cifar(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # Scale pixel intensity\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "    \n",
    "    # Reshape\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32)\n",
    "    x_test = x_test.reshape(-1, 3, 32, 32)\n",
    "    \n",
    "    x_train = np.swapaxes(x_train, 1, 3)\n",
    "    x_test = np.swapaxes(x_test, 1, 3)\n",
    "    \n",
    "    return (x_train.astype(np.float32), \n",
    "            y_train.astype(np.int32), \n",
    "            x_test.astype(np.float32), \n",
    "            y_test.astype(np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for serving the data to the CNN \n",
    "The data will be fed to the NN in minibatches. A minibatch is a set of examples taken from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_from(X, y, batchsize=BATCHSIZE, shuffle=False):\n",
    "    if len(X) != len(y):\n",
    "        raise Exception(\"The length of X {} and y {} don't match\".format(len(X), len(y)))\n",
    "        \n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y)\n",
    "    \n",
    "    for i in range(0, len(X), batchsize):\n",
    "        yield X[i:i + batchsize], y[i:i + batchsize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is desirable during training to shuffle the data so that each minibatch isn't always the same. During evaluation or testing though this is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):\n",
    "    index = np.arange(len(X))\n",
    "    np.random.shuffle(index)\n",
    "    return X[index], y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and initialising the model\n",
    "Below is the definition of our model. It isn't very deep and only has 2 convolutional layers. Both convolution layers have 50 kernels each with a dimension of 3 by 3. The first convolution layer uses relu activation and the second convolution layer carries out max pooling before using relu activation. After that we need to reshape our Tensor into 2D matrix with the first dimension being the size of our batch. After that we pass it into a fully connected layer of 512 nodes. This layer uses relu actionation. Finally we introduce our final dense layer which has 10 outpus, one for each of our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_input, n_classes=N_CLASSES, data_format='channels_last'):\n",
    "    conv1 = tf.layers.conv2d(model_input, \n",
    "                             filters=50, \n",
    "                             kernel_size=(3, 3), \n",
    "                             padding='same', \n",
    "                             data_format=data_format,\n",
    "                             activation=tf.nn.relu)\n",
    "    conv2 = tf.layers.conv2d(conv1, \n",
    "                             filters=50, \n",
    "                             kernel_size=(3, 3), \n",
    "                             padding='same', \n",
    "                             data_format=data_format,\n",
    "                             activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(conv2, \n",
    "                                    pool_size=(2, 2), \n",
    "                                    strides=(2, 2), \n",
    "                                    padding='valid', \n",
    "                                    data_format=data_format)\n",
    "    flatten = tf.reshape(pool1, shape=[-1, 50*16*16])\n",
    "    fc1 = tf.layers.dense(flatten, 512, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(fc1, n_classes, name='output')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important element in training neural network is defining the loss function and optimisation to use. Here we are using cross entropy as our loss function and stochastic gradient descent(SGD) with momentum as our optimisation function. SGD is the standard optimisation method for Deep Learning. The two parameters we have to define are the learning rate and momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_training(m, labels, learning_rate=LR, momentum=MOMENTUM):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=m, labels=y)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum)\n",
    "    return optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the CIFAR10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Data into format for library\n",
    "x_train, y_train, x_test, y_test = prepare_cifar(*load_cifar())\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print(x_train.dtype, x_test.dtype, y_train.dtype, y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model\n",
    "Our images (X) are 32x32 and have 3 color channels. Here we are defining our Tensor with channel color last. The y represents the class output which goes from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Place-holders\n",
    "X = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "# Initialise model\n",
    "model = create_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model = init_model_training(model, y)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Accuracy logging\n",
    "correct = tf.nn.in_top_k(model, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "Here we train the model for the number of epochs we defined at the top of the notebook. During this process we execute the forward pass, calculate the loss and then propagate the error backwards and update the weights. This can take a considerable amount of time dependings on the computational resources you have at your disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for j in range(EPOCHS):\n",
    "    for data, label in minibatch_from(x_train, y_train, shuffle=True):\n",
    "        sess.run(train_model, feed_dict={X: data, y: label})\n",
    "    # Log\n",
    "    acc_train = sess.run(accuracy, feed_dict={X: data, y: label})\n",
    "    print(j, \"Train accuracy:\", acc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_guess = list()\n",
    "for data, label in minibatch_from(x_test, y_test):\n",
    "    pred = tf.argmax(model,1)\n",
    "    output = sess.run(pred, feed_dict={X: data})\n",
    "    y_guess.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", sum(np.concatenate(y_guess) == y_test)/float(len(y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
