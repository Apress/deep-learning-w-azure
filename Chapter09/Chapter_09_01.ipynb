{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Batch Shipyard\n",
    "This notebook will set up everything for the tutorial. This notebook assumes that nothing has been set up previously and will create everything from scratch. The necessary steps are broken up into the following sections:\n",
    "\n",
    "**Note:** By using these notebooks on GPU VMs, you agree to the [NVIDIA Software License](http://www.nvidia.com/content/DriverDownload-March2009/licence.php?lang=us).\n",
    "\n",
    "* [Install tools and dependencies](#section1)\n",
    "* [Azure account login](#section2)\n",
    "* [Setup](#section3)\n",
    "* [Create Azure resources](#section4)\n",
    "* [Define our model](#model)\n",
    "* [Batch Shipyard Configuration](#section5)\n",
    "* [Create Azure Batch Pool](#section6)\n",
    "* [Configure Job](#section7)\n",
    "* [Submit Job](#section8)\n",
    "* [Delete Job and Deallocate Pool](#section9)\n",
    "* [Delete Azure resources](#section10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## Install tools and dependencies\n",
    "Install Batch Shipyard and Azure CLI 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Azure/batch-shipyard.git --branch 3.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally you would use `install.sh` or `install.cmd` helper scripts to install, but due to the Notebook environment, we will instead install with the `requirements.txt` file directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install blobxfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install ruamel.yaml pykwalify==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -r batch-shipyard/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure CLI 2.0 will also be installed to help us in provisioning Azure Batch and Storage accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -I azure-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an alias for invoking Batch Shipyard which points to a `config` directory which will hold our yaml config files (this directory will be created at a later step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd=!pwd\n",
    "%alias shipyard SHIPYARD_CONFIGDIR=config python {pwd[0]}/batch-shipyard/shipyard.py %l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipyard --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Azure account login\n",
    "The command below will initiate a login to your Azure account. You will see a url to browse to where you will enter the specified code. This will log you into the Azure account within the Azure CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have multiple subscriptions you can select the one you need with the command below. This will not be necessary for your assigned Azure Pass account for the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_subscription = \"<YOUR SUBSCRIPTION>\" # Replace with the name of your subscription\n",
    "!az account set --subscription \"$selected_subscription\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you cannot login with the Azure CLI, you can create Azure Batch and Storage accounts on the [Azure Portal](https://portal.azure.com).\n",
    "- [Instructions for creating an Azure Batch Account](https://docs.microsoft.com/en-us/azure/batch/batch-account-create-portal#batch-service-mode)\n",
    "- [Instructions for creating an Azure Storage Account](https://docs.microsoft.com/en-us/azure/storage/storage-create-storage-account#create-a-storage-account) (You can create an \"Auto Storage\" account at the same time as your Batch Account on the portal instead)\n",
    "\n",
    "Please pay attention to special instructions regarding Azure Portal created accounts below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## Setup\n",
    "First we will need to register the Azure Batch service as a resource provider for the Azure subscription. The following will do so and poll until the registration process has completed. This will take approximately 30 seconds to complete.\n",
    "\n",
    "**Note:** This registration process needs to be performed only once for the Azure subscription. If you created your Azure Batch account via the Azure Portal, you do not need to perform this action as it is done automatically for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# register resource provider with subscription\n",
    "print('Registering Microsoft.Batch with subscription. Please be patient for this process.')\n",
    "!az provider register -n Microsoft.Batch\n",
    "\n",
    "# poll until registration completed\n",
    "while True:\n",
    "    status = !az provider show -n Microsoft.Batch -o table\n",
    "    if status[-1].split()[-1] == 'Registered':\n",
    "        print('\\n'.join(status))\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the various names for the resources needed to run Azure Batch jobs.\n",
    "\n",
    "**Note:** If you manually created your accounts on the Azure Portal, you will need to modify `GROUP_NAME`, `BATCH_ACCOUNT_NAME` and `STORAGE_ACCOUNT_NAME` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import json\n",
    "\n",
    "def write_yaml_to_file(yaml_dict, filename):\n",
    "    \"\"\" Simple function to write YAML dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        yaml.dump(yaml_dict, outfile)\n",
    "\n",
    "LOCATION = 'eastus' # We are setting everything up in East US\n",
    "                    # Be aware that you need to set things up in a region that has GPU VMs (N-Series)\n",
    "\n",
    "# Tensorflow image\n",
    "IMAGE_NAME = \"tensorflow/tensorflow:1.8.0-gpu-py3\"\n",
    "\n",
    "short_uuid = str(uuid.uuid4())[:8]\n",
    "GROUP_NAME = \"batch{uuid}rg\".format(uuid=short_uuid)\n",
    "BATCH_ACCOUNT_NAME = \"batch{uuid}ba\".format(uuid=short_uuid)\n",
    "STORAGE_ACCOUNT_NAME = \"batch{uuid}st\".format(uuid=short_uuid)\n",
    "STORAGE_ALIAS = \"mystorageaccount\"\n",
    "STORAGE_ENDPOINT = \"core.windows.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## Create Azure resources\n",
    "### Create Resource Group\n",
    "Azure encourages the use of resource groups to organise all the Azure components you deploy. That way it is easier to find them but also we can deleted a number of resources simply by deleting the Resource Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az group create -n $GROUP_NAME -l $LOCATION -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batch and Storage accounts\n",
    "Here we simply crate the Batch and Storage accounts. Once we have created the accounts we can the use the Azure CLI to query them and obtain the **batch_account_key**, **batch_service_url** and **storage_account_key** which we will need for our Batch Shipyard configuration files later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = !az storage account create -l $LOCATION -n $STORAGE_ACCOUNT_NAME -g $GROUP_NAME --sku Standard_LRS\n",
    "print('Storage account {} provisioning state: {}'.format(STORAGE_ACCOUNT_NAME, json.loads(''.join(json_data))['provisioningState']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = !az batch account create -l $LOCATION -n $BATCH_ACCOUNT_NAME -g $GROUP_NAME --storage-account $STORAGE_ACCOUNT_NAME\n",
    "print('Batch account {} provisioning state: {}'.format(BATCH_ACCOUNT_NAME, json.loads(''.join(json_data))['provisioningState']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we retrieve the **batch_account_key**, **batch_service_url** and **storage_account_key** which we will need for the Batch Shipyard configuration files further down.\n",
    "\n",
    "**Note:** If you created your Batch and Storage accounts in the Azure Portal, you will need to retrieve your keys in the Portal. Then set `batch_account_key`, `batch_service_url`, and `storage_account_key` to their appropriate values instead of the Azure CLI callouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = !az batch account keys list -n $BATCH_ACCOUNT_NAME -g $GROUP_NAME\n",
    "batch_account_key = json.loads(''.join(json_data))['primary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = !az batch account list -g $GROUP_NAME\n",
    "batch_service_url = 'https://'+json.loads(''.join(json_data))[0]['accountEndpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = !az storage account keys list -n $STORAGE_ACCOUNT_NAME -g $GROUP_NAME\n",
    "storage_account_key = json.loads(''.join(json_data))[0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_information = {\n",
    "    \"IMAGE_NAME\": IMAGE_NAME,\n",
    "    \"LOCATION\": LOCATION,\n",
    "    \"resource_group\": GROUP_NAME,\n",
    "    \"STORAGE_ALIAS\": STORAGE_ALIAS,\n",
    "    \"storage_account_key\": storage_account_key,\n",
    "    \"storage_account_name\": STORAGE_ACCOUNT_NAME,\n",
    "}\n",
    "write_yaml_to_file(account_information, 'account_information.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model'></a>\n",
    "## Define Our Model\n",
    "The file below contains a simple CNN written in Keras. It will load the CIFAR 10 data and then train the model for a number of epochs and then evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cifar10_cnn.py\n",
    "'''Train a VGG-like CNN on the CIFAR10 small images dataset.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import logging\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Parameters\n",
    "EPOCHS = 30\n",
    "BATCHSIZE = 64\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "N_CLASSES = 10 # There are 10 classes in the CIFAR10 dataset\n",
    "DATA_FORMAT = 'channels_first'\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def read_pickle(src):\n",
    "    with open(src, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin1')\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_cifar():\n",
    "    \"\"\" Read data\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info('Preparing train set...')\n",
    "    train_list = [read_pickle('./cifar-10-batches-py/data_batch_{0}'.format(i)) for i in range(1, 6)]\n",
    "    x_train = np.concatenate([t['data'] for t in train_list])\n",
    "    y_train = np.concatenate([t['labels'] for t in train_list])\n",
    "    \n",
    "    logger.info('Preparing test set...')\n",
    "    tst = read_pickle('./cifar-10-batches-py/test_batch')\n",
    "    x_test = tst['data']\n",
    "    y_test = np.asarray(tst['labels'])\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def prepare_cifar(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # Scale pixel intensity\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "    \n",
    "    # Reshape\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32)\n",
    "    x_test = x_test.reshape(-1, 3, 32, 32)\n",
    "    \n",
    "    return (x_train.astype(np.float32), \n",
    "            y_train.astype(np.int32), \n",
    "            x_test.astype(np.float32), \n",
    "            y_test.astype(np.int32))\n",
    "\n",
    "\n",
    "def load_cifar(src=\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"):\n",
    "    \"\"\" Load CIFAR10 Dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return process_cifar()\n",
    "    except FileNotFoundError:\n",
    "        logger.info('Data does not exist. Downloading ' + src)\n",
    "        fname, h = urlretrieve(src, './delete.me')\n",
    "        logger.info('Extracting files...')\n",
    "        with tarfile.open(fname) as tar:\n",
    "            tar.extractall()\n",
    "        os.remove(fname)\n",
    "    return process_cifar()\n",
    "\n",
    "\n",
    "def init_model_training(m, labels, learning_rate=LR, momentum=MOMENTUM):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=m, labels=labels)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum)\n",
    "    return optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    index = np.arange(len(X))\n",
    "    np.random.shuffle(index)\n",
    "    return X[index], y[index]\n",
    "\n",
    "\n",
    "def minibatch_from(X, y, batchsize=BATCHSIZE, shuffle=False):\n",
    "    if len(X) != len(y):\n",
    "        raise Exception(\"The length of X {} and y {} don't match\".format(len(X), len(y)))\n",
    "        \n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y)\n",
    "    \n",
    "    for i in range(0, len(X), batchsize):\n",
    "        yield X[i:i + batchsize], y[i:i + batchsize]\n",
    "        \n",
    "\n",
    "def main(epochs=EPOCHS, lr=LR, mb_size=BATCHSIZE, data_format=DATA_FORMAT):\n",
    "    logger.info('Learning Rate: {} Minibatch Size: {} Epochs: {}'.format(lr, mb_size, epochs))\n",
    "    \n",
    "    logger.info('Loading data....')\n",
    "    # Data into format for library\n",
    "    x_train, y_train, x_test, y_test = prepare_cifar(*load_cifar())\n",
    "    logger.info('Data shape {}'.format(str((x_train.shape, x_test.shape, y_train.shape, y_test.shape))))\n",
    "    logger.info('Data types {}'.format(str((x_train.dtype, x_test.dtype, y_train.dtype, y_test.dtype))))\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    # Place-holders\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 3, 32, 32])\n",
    "    y = tf.placeholder(tf.int32, shape=[None])\n",
    "    training = tf.placeholder(tf.bool)  # Indicator for dropout layer\n",
    "    \n",
    "    # Define model\n",
    "    # Block 1\n",
    "    conv1_1 = tf.layers.conv2d(X, \n",
    "                               filters=64, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    conv1_2 = tf.layers.conv2d(conv1_1, \n",
    "                               filters=64, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    pool1_1 = tf.layers.max_pooling2d(conv1_2, \n",
    "                                      pool_size=(2, 2), \n",
    "                                      strides=(2, 2), \n",
    "                                      padding='valid', \n",
    "                                      data_format=data_format)\n",
    "    # Block 2\n",
    "    conv2_1 = tf.layers.conv2d(pool1_1, \n",
    "                               filters=128, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    conv2_2 = tf.layers.conv2d(conv2_1, \n",
    "                               filters=128, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    pool2_1 = tf.layers.max_pooling2d(conv2_2, \n",
    "                                      pool_size=(2, 2), \n",
    "                                      strides=(2, 2), \n",
    "                                      padding='valid', \n",
    "                                      data_format=data_format)\n",
    "\n",
    "    # Block 3\n",
    "    conv3_1 = tf.layers.conv2d(pool2_1, \n",
    "                               filters=256, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    conv3_2 = tf.layers.conv2d(conv3_1, \n",
    "                               filters=256, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    conv3_3 = tf.layers.conv2d(conv3_2, \n",
    "                               filters=256, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    pool3_1 = tf.layers.max_pooling2d(conv3_3, \n",
    "                                      pool_size=(2, 2), \n",
    "                                      strides=(2, 2), \n",
    "                                      padding='valid', \n",
    "                                      data_format=data_format)\n",
    "\n",
    "    relu2 = tf.nn.relu(pool3_1)\n",
    "    flatten = tf.reshape(relu2, shape=[-1, 256*4*4])\n",
    "    fc1 = tf.layers.dense(flatten, 4096, activation=tf.nn.relu)\n",
    "    drop1 = tf.layers.dropout(fc1, 0.5, training=training)\n",
    "    fc2 = tf.layers.dense(drop1, 4096, activation=tf.nn.relu)\n",
    "    drop2 = tf.layers.dropout(fc2, 0.5, training=training)\n",
    "    model = tf.layers.dense(drop2, N_CLASSES, name='output')\n",
    "\n",
    "    train_model = init_model_training(model, y, learning_rate=lr)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Accuracy logging\n",
    "    correct = tf.nn.in_top_k(model, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    logger.info('Training model...')\n",
    "    \n",
    "    # Train model\n",
    "    for j in range(epochs):\n",
    "        for data, label in minibatch_from(x_train, y_train, shuffle=True, batchsize=mb_size):\n",
    "            sess.run(train_model, feed_dict={X: data, y: label, training: True})\n",
    "        # Log\n",
    "        acc_train = sess.run(accuracy, feed_dict={X: data, y: label, training: True})\n",
    "        logger.info(\"{} | Train accuracy: {}\".format(j, acc_train))\n",
    "    \n",
    "    logger.info('Evaluating model...')\n",
    "    y_guess = list()\n",
    "    for data, label in minibatch_from(x_test, y_test):\n",
    "        pred = tf.argmax(model,1)\n",
    "        output = sess.run(pred, feed_dict={X: data, training: False})\n",
    "        y_guess.append(output)\n",
    "    logger.info(\"Accuracy: {}\".format(sum(np.concatenate(y_guess) == y_test)/float(len(y_test))))\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    logger.info('Starting script....')\n",
    "    parser = argparse.ArgumentParser(description='Script to train VGG-like model on CIFAR10 dataset')\n",
    "    parser.add_argument('--lr', help='Specify learning rate', type=float, default=LR)\n",
    "    parser.add_argument('--mb_size', help='Minibatch size', type=int, default=BATCHSIZE)\n",
    "    parser.add_argument('--epochs', help='Number of epochs to train for', type=int, default=EPOCHS)\n",
    "    args = parser.parse_args()\n",
    "    main(epochs=args.epochs, lr=args.lr, mb_size=args.mb_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## Batch Shiyard configuration\n",
    "In order to execute a job on Batch Shipyard you need a minimum of four configuration files. We will set three of them here and leave the job one for later.\n",
    "* [credentials](#credentials)\n",
    "* [configuration](#configuration)\n",
    "* [pool](#pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CONTAINER = 'input'\n",
    "UPLOAD_DIR = 'dist_upload'\n",
    "\n",
    "!rm -rf $UPLOAD_DIR\n",
    "!mkdir -p $UPLOAD_DIR\n",
    "!mv cifar10_cnn.py $UPLOAD_DIR\n",
    "!chmod 777 $UPLOAD_DIR/cifar10_cnn.py\n",
    "!ls -alF $UPLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='credentials'></a>\n",
    "### Credentials\n",
    "Here we define all the credentials necessary for Batch Shipyard to connect to Azure for resource provisioning and executing our jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"credentials\": {\n",
    "        \"batch\": {\n",
    "            \"account_key\": batch_account_key,\n",
    "            \"account_service_url\": batch_service_url\n",
    "        },\n",
    "        \"storage\": {\n",
    "            STORAGE_ALIAS : {\n",
    "                    \"account\": STORAGE_ACCOUNT_NAME,\n",
    "                    \"account_key\": storage_account_key,\n",
    "                    \"endpoint\": STORAGE_ENDPOINT\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='configuration'></a>\n",
    "### Configuration\n",
    "The config mainly contains the configuration for Batch Shipyard. Here we simply define the storage alias that Batch Shipyard should use as well as the Docker image to use. We also tell Batch shipyard to upload everything it finds in the `dist_upload` directory to the `input` blob container. The `dist_upload` directory contains the script we wrote earlier which we train and evaluate our deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_shipyard\": {\n",
    "        \"storage_account_settings\": STORAGE_ALIAS,\n",
    "    },\n",
    "   \"global_resources\": {\n",
    "        \"docker_images\": [\n",
    "            IMAGE_NAME\n",
    "        ],\n",
    "        \"files\": [\n",
    "            {\n",
    "                \"source\": {\n",
    "                    \"path\": \"dist_upload\"\n",
    "                },\n",
    "                \"destination\": {\n",
    "                    \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                    \"data_transfer\": {\n",
    "                        \"remote_path\": INPUT_CONTAINER\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pool'></a>\n",
    "### Pool\n",
    "This is where we define the properties of the compute pool we wish to create. The configuration below creates a pool that is made up of a single NC6 VM running Ubuntu 16.04. If you wish to run a job that uses GPU accelerated compute, as we will be doing for these notebooks, then you will need to choose a VM from the NC series. Here we will allocate 1 `STANDARD_NC6` instances. You may opt to change the `vm_count` from `dedicated` to `low_priority` to save on costs, but please note that you may be pre-empted at any time. It is recommended to use `dedicated` for this tutorial to avoid running into those issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POOL_ID = 'gpupool'\n",
    "\n",
    "pool = {\n",
    "    \"pool_specification\": {\n",
    "        \"id\": POOL_ID,\n",
    "        \"vm_configuration\": {\n",
    "            \"platform_image\": {\n",
    "                \"publisher\": \"Canonical\",\n",
    "                \"offer\": \"UbuntuServer\",\n",
    "                \"sku\": \"16.04-LTS\",\n",
    "                \"native\": False,\n",
    "            },\n",
    "        },\n",
    "        \"vm_size\": \"STANDARD_NC6\",\n",
    "        \"vm_count\": {\n",
    "            \"dedicated\": 1\n",
    "        },\n",
    "        \"ssh\": {\n",
    "            \"username\": \"shipyard\",\n",
    "            \"generate_docker_tunnel_script\": True\n",
    "        },\n",
    "        \"reboot_on_start_task_failed\": False,\n",
    "        \"block_until_all_global_resources_loaded\": True,\n",
    "        \"transfer_files_on_pool_creation\": True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p config # Create config file directory where we will store all our Batch Shipyard configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_yaml_to_file(credentials, os.path.join('config', 'credentials.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_yaml_to_file(config, os.path.join('config', 'config.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_yaml_to_file(pool, os.path.join('config', 'pool.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IMAGE_NAME = \"{}\"'.format(IMAGE_NAME))\n",
    "print('GROUP_NAME = \"{}\"'.format(GROUP_NAME))\n",
    "print('LOCATION = \"{}\"'.format(LOCATION))\n",
    "print('BATCH_ACCOUNT_NAME = \"{}\"'.format(BATCH_ACCOUNT_NAME))\n",
    "print('batch_account_key = \"{}\"'.format(batch_account_key))\n",
    "print('batch_service_url = \"{}\"'.format(batch_service_url))\n",
    "print('STORAGE_ACCOUNT_NAME = \"{}\"'.format(STORAGE_ACCOUNT_NAME))\n",
    "print('STORAGE_ALIAS = \"{}\"'.format(STORAGE_ALIAS))\n",
    "print('storage_account_key = \"{}\"'.format(storage_account_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## Create Azure Batch Pool\n",
    "Before we do anything we need to create the pool for Batch Shipyard jobs to run on. This can take a little bit of time so please be patient while the compute nodes are allocated from the Azure Cloud and the Docker images are pre-loaded on to the compute nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shipyard pool add -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pool is created we can confirm everything by running the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipyard pool list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## Configure Job\n",
    "As before the dictionary below defines the job we will execute. Here we are pulling the script from the blob container that we uploaded during pool creation into `AZ_BATCH_TASK_WORKING_DIR` and executing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_ID = 'run_cifar10' # This should be changed per task\n",
    "JOB_ID = 'tf-training-job'\n",
    "COMMAND = '/bin/bash -c \"python cifar10_cnn.py\"'\n",
    "\n",
    "jobs = {\n",
    "    \"job_specifications\": [\n",
    "        {\n",
    "            \"id\": JOB_ID,\n",
    "            \"allow_run_on_missing_image\": False,\n",
    "            \"gpu\": True,\n",
    "            \"tasks\": [\n",
    "                {\n",
    "                    \"id\": TASK_ID,\n",
    "                    \"docker_image\": IMAGE_NAME,\n",
    "                    \"command\": COMMAND,\n",
    "                    \"gpu\": True,\n",
    "                    \"input_data\": {\n",
    "                    \"azure_storage\": [\n",
    "                        {\n",
    "                            \"storage_account_settings\": STORAGE_ALIAS,\n",
    "                            \"remote_path\": INPUT_CONTAINER,\n",
    "                            \"local_path\": \"$AZ_BATCH_TASK_WORKING_DIR\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_yaml_to_file(jobs, os.path.join('config', 'jobs.yaml'))\n",
    "print(yaml.dump(jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "## Submit Job\n",
    "Now that we have confirmed everything is working we can execute our job using the command below. The tail switch at the end will stream stdout from the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shipyard jobs add --tail stdout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can stream any of the text files that are on the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipyard data files stream -v --filespec $JOB_ID,$TASK_ID,stderr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipyard jobs list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "## Delete job and deallocate pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipyard jobs del -y --termtasks --wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipyard pool del -y --wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section10'></a>\n",
    "## Delete Azure resources\n",
    "Once you have deleted the pool all that remains is the storage account and the Batch account.\n",
    "\n",
    "Note that you do not need to delete your batch and storage accounts.\n",
    "- You will only be billed in Batch for pools for compute node time and data egress. If you do not have any active pools with nodes in them, you will not be billed for anything.\n",
    "- Storage costs include data stored in blobs and transactions. For the examples in these notebooks, the cost will be very small.\n",
    "\n",
    "However, if you wish to delete your accounts, you can do so by deleting the resource group containing the accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az group delete -n $GROUP_NAME --yes --verbose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
