{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Batch AI\n",
    "This notebook will set up everything for the tutorial. This notebook assumes that nothing has been set up previously and will create everything from scratch. The necessary steps are broken up into the following sections:\n",
    "\n",
    "**Note:** If you have run the Batch Shipyard example you can skip the installation of the Azure CLI\n",
    "\n",
    "* [Install tools and dependencies](#section1)\n",
    "* [Azure account login](#section2)\n",
    "* [Setup](#section3)\n",
    "* [Create Azure resources](#section4)\n",
    "* [Define our model](#model)\n",
    "* [Create Azure Batch AI Cluster](#section6)\n",
    "* [Configure Job](#section7)\n",
    "* [Submit Job](#section8)\n",
    "* [Delete Job and Deallocate Pool](#section9)\n",
    "* [Delete Azure resources](#section10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## Install tools and dependencies\n",
    "Azure CLI 2.0 will also be installed to help us in provisioning Azure Storage accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -I azure-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az provider register -n Microsoft.BatchAI\n",
    "!az provider register -n Microsoft.Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that everything is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Azure account login\n",
    "The command below will initiate a login to your Azure account. You will see a url to browse to where you will enter the specified code. This will log you into the Azure account within the Azure CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have multiple subscriptions you can select the one you need with the command below. This will not be necessary for your assigned Azure Pass account for the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_subscription = \"<YOUR SUBSCRIPTION>\" # Replace with the name of your subscription\n",
    "!az account set --subscription \"$selected_subscription\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you cannot login with the Azure CLI, you can create Storage accounts on the [Azure Portal](https://portal.azure.com).\n",
    "- [Instructions for creating an Azure Storage Account](https://docs.microsoft.com/en-us/azure/storage/storage-create-storage-account#create-a-storage-account) (You can create an \"Auto Storage\" account at the same time as your Batch Account on the portal instead)\n",
    "\n",
    "Please pay attention to special instructions regarding Azure Portal created accounts below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## Setup\n",
    "Now we will define the various names for the resources needed to run Batch AI jobs.\n",
    "\n",
    "**Note:** If you manually created your accounts on the Azure Portal, you will need to modify `GROUP_NAME` and `STORAGE_ACCOUNT_NAME` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import json\n",
    "\n",
    "def write_json_to_file(json_dict, filename):\n",
    "    \"\"\" Simple function to write JSON dictionaries to files\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "\n",
    "LOCATION = 'eastus' # We are setting everything up in East US\n",
    "                    # Be aware that you need to set things up in a region that has GPU VMs (N-Series)\n",
    "\n",
    "# Tensorflow image\n",
    "IMAGE_NAME = \"tensorflow/tensorflow:1.8.0-gpu-py3\"\n",
    "\n",
    "short_uuid = str(uuid.uuid4())[:8]\n",
    "GROUP_NAME = \"batch{uuid}rg\".format(uuid=short_uuid)\n",
    "FILESHARE_NAME = \"batch{uuid}share\".format(uuid=short_uuid)\n",
    "SHARE_DIRECTORY = \"cnn_example\"\n",
    "STORAGE_ACCOUNT_NAME = \"batch{uuid}st\".format(uuid=short_uuid)\n",
    "WORKSPACE='workspace'\n",
    "EXPERIMENT='experiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## Create Azure resources\n",
    "### Create Resource Group\n",
    "Azure encourages the use of resource groups to organise all the Azure components you deploy. That way it is easier to find them but also we can deleted a number of resources simply by deleting the Resource Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az group create -n $GROUP_NAME -l $LOCATION -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Storage account\n",
    "Here we simply create the Storage accounts. Once we have created the accounts we can the use the Azure CLI to query it and obtain the **storage_account_key** which we will need for our Batch AI configuration files later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = !az storage account create -l $LOCATION -n $STORAGE_ACCOUNT_NAME -g $GROUP_NAME --sku Standard_LRS\n",
    "print('Storage account {} provisioning state: {}'.format(STORAGE_ACCOUNT_NAME, json.loads(''.join(json_data))['provisioningState']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az storage share create --account-name $STORAGE_ACCOUNT_NAME --name $FILESHARE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we retrieve the **storage_account_key** which we will need for the Batch AI configuration files further down.\n",
    "\n",
    "**Note:** If you created your Storage account in the Azure Portal, you will need to retrieve your keys in the Portal. Then set `batch_account_key` to the appropriate value instead of the Azure CLI callouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = !az storage account keys list -n $STORAGE_ACCOUNT_NAME -g $GROUP_NAME\n",
    "storage_account_key = json.loads(''.join(json_data))[0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az configure --defaults group=$GROUP_NAME\n",
    "!az configure --defaults location=$LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the directory on the fileshare we create where we will save our script to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az storage directory create --share-name $FILESHARE_NAME  --name $SHARE_DIRECTORY \\\n",
    "--account-name $STORAGE_ACCOUNT_NAME --account-key $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the account information so that we can retrieve it if something happens to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_information = {\n",
    "    \"IMAGE_NAME\": IMAGE_NAME,\n",
    "    \"LOCATION\": LOCATION,\n",
    "    \"resource_group\": GROUP_NAME,\n",
    "    \"storage_account_key\": storage_account_key,\n",
    "    \"storage_account_name\": STORAGE_ACCOUNT_NAME,\n",
    "}\n",
    "write_json_to_file(account_information, 'account_information.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model'></a>\n",
    "## Define Our Model\n",
    "The file below contains a simple CNN written in Keras. It will load the CIFAR 10 data and then train the model for a number of epochs and then evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cifar10_cnn.py\n",
    "'''Train a VGG-like CNN on the CIFAR10 small images dataset.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import logging\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Parameters\n",
    "EPOCHS = 30\n",
    "BATCHSIZE = 64\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "N_CLASSES = 10 # There are 10 classes in the CIFAR10 dataset\n",
    "DATA_FORMAT = 'channels_first'\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def read_pickle(src):\n",
    "    with open(src, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin1')\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_cifar():\n",
    "    \"\"\" Read data\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info('Preparing train set...')\n",
    "    train_list = [read_pickle('./cifar-10-batches-py/data_batch_{0}'.format(i)) for i in range(1, 6)]\n",
    "    x_train = np.concatenate([t['data'] for t in train_list])\n",
    "    y_train = np.concatenate([t['labels'] for t in train_list])\n",
    "    \n",
    "    logger.info('Preparing test set...')\n",
    "    tst = read_pickle('./cifar-10-batches-py/test_batch')\n",
    "    x_test = tst['data']\n",
    "    y_test = np.asarray(tst['labels'])\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def prepare_cifar(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # Scale pixel intensity\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "    \n",
    "    # Reshape\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32)\n",
    "    x_test = x_test.reshape(-1, 3, 32, 32)\n",
    "    \n",
    "    return (x_train.astype(np.float32), \n",
    "            y_train.astype(np.int32), \n",
    "            x_test.astype(np.float32), \n",
    "            y_test.astype(np.int32))\n",
    "\n",
    "\n",
    "def load_cifar(src=\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"):\n",
    "    \"\"\" Load CIFAR10 Dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return process_cifar()\n",
    "    except FileNotFoundError:\n",
    "        logger.info('Data does not exist. Downloading ' + src)\n",
    "        fname, h = urlretrieve(src, './delete.me')\n",
    "        logger.info('Extracting files...')\n",
    "        with tarfile.open(fname) as tar:\n",
    "            tar.extractall()\n",
    "        os.remove(fname)\n",
    "    return process_cifar()\n",
    "\n",
    "\n",
    "def init_model_training(m, labels, learning_rate=LR, momentum=MOMENTUM):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=m, labels=labels)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum)\n",
    "    return optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "def shuffle_data(X, y):\n",
    "    index = np.arange(len(X))\n",
    "    np.random.shuffle(index)\n",
    "    return X[index], y[index]\n",
    "\n",
    "\n",
    "def minibatch_from(X, y, batchsize=BATCHSIZE, shuffle=False):\n",
    "    if len(X) != len(y):\n",
    "        raise Exception(\"The length of X {} and y {} don't match\".format(len(X), len(y)))\n",
    "        \n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y)\n",
    "    \n",
    "    for i in range(0, len(X), batchsize):\n",
    "        yield X[i:i + batchsize], y[i:i + batchsize]\n",
    "        \n",
    "\n",
    "def main(epochs=EPOCHS, lr=LR, mb_size=BATCHSIZE, data_format=DATA_FORMAT):\n",
    "    logger.info('Learning Rate: {} Minibatch Size: {} Epochs: {}'.format(lr, mb_size, epochs))\n",
    "    \n",
    "    logger.info('Loading data....')\n",
    "    # Data into format for library\n",
    "    x_train, y_train, x_test, y_test = prepare_cifar(*load_cifar())\n",
    "    logger.info('Data shape {}'.format(str((x_train.shape, x_test.shape, y_train.shape, y_test.shape))))\n",
    "    logger.info('Data types {}'.format(str((x_train.dtype, x_test.dtype, y_train.dtype, y_test.dtype))))\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    # Place-holders\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 3, 32, 32])\n",
    "    y = tf.placeholder(tf.int32, shape=[None])\n",
    "    training = tf.placeholder(tf.bool)  # Indicator for dropout layer\n",
    "    \n",
    "    # Define model\n",
    "    # Block 1\n",
    "    conv1_1 = tf.layers.conv2d(X, \n",
    "                               filters=64, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    conv1_2 = tf.layers.conv2d(conv1_1, \n",
    "                               filters=64, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    pool1_1 = tf.layers.max_pooling2d(conv1_2, \n",
    "                                      pool_size=(2, 2), \n",
    "                                      strides=(2, 2), \n",
    "                                      padding='valid', \n",
    "                                      data_format=data_format)\n",
    "    # Block 2\n",
    "    conv2_1 = tf.layers.conv2d(pool1_1, \n",
    "                               filters=128, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    conv2_2 = tf.layers.conv2d(conv2_1, \n",
    "                               filters=128, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    pool2_1 = tf.layers.max_pooling2d(conv2_2, \n",
    "                                      pool_size=(2, 2), \n",
    "                                      strides=(2, 2), \n",
    "                                      padding='valid', \n",
    "                                      data_format=data_format)\n",
    "\n",
    "    # Block 3\n",
    "    conv3_1 = tf.layers.conv2d(pool2_1, \n",
    "                               filters=256, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    conv3_2 = tf.layers.conv2d(conv3_1, \n",
    "                               filters=256, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    conv3_3 = tf.layers.conv2d(conv3_2, \n",
    "                               filters=256, \n",
    "                               kernel_size=(3, 3), \n",
    "                               padding='same', \n",
    "                               data_format=data_format,\n",
    "                               activation=tf.nn.relu)\n",
    "    pool3_1 = tf.layers.max_pooling2d(conv3_3, \n",
    "                                      pool_size=(2, 2), \n",
    "                                      strides=(2, 2), \n",
    "                                      padding='valid', \n",
    "                                      data_format=data_format)\n",
    "\n",
    "    relu2 = tf.nn.relu(pool3_1)\n",
    "    flatten = tf.reshape(relu2, shape=[-1, 256*4*4])\n",
    "    fc1 = tf.layers.dense(flatten, 4096, activation=tf.nn.relu)\n",
    "    drop1 = tf.layers.dropout(fc1, 0.5, training=training)\n",
    "    fc2 = tf.layers.dense(drop1, 4096, activation=tf.nn.relu)\n",
    "    drop2 = tf.layers.dropout(fc2, 0.5, training=training)\n",
    "    model = tf.layers.dense(drop2, N_CLASSES, name='output')\n",
    "\n",
    "    train_model = init_model_training(model, y, learning_rate=lr)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Accuracy logging\n",
    "    correct = tf.nn.in_top_k(model, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    logger.info('Training model...')\n",
    "    \n",
    "    # Train model\n",
    "    for j in range(epochs):\n",
    "        for data, label in minibatch_from(x_train, y_train, shuffle=True, batchsize=mb_size):\n",
    "            sess.run(train_model, feed_dict={X: data, y: label, training: True})\n",
    "        # Log\n",
    "        acc_train = sess.run(accuracy, feed_dict={X: data, y: label, training: True})\n",
    "        logger.info(\"{} | Train accuracy: {}\".format(j, acc_train))\n",
    "    \n",
    "    logger.info('Evaluating model...')\n",
    "    y_guess = list()\n",
    "    for data, label in minibatch_from(x_test, y_test):\n",
    "        pred = tf.argmax(model,1)\n",
    "        output = sess.run(pred, feed_dict={X: data, training: False})\n",
    "        y_guess.append(output)\n",
    "    logger.info(\"Accuracy: {}\".format(sum(np.concatenate(y_guess) == y_test)/float(len(y_test))))\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    logger.info('Starting script....')\n",
    "    parser = argparse.ArgumentParser(description='Script to train VGG-like model on CIFAR10 dataset')\n",
    "    parser.add_argument('--lr', help='Specify learning rate', type=float, default=LR)\n",
    "    parser.add_argument('--mb_size', help='Minibatch size', type=int, default=BATCHSIZE)\n",
    "    parser.add_argument('--epochs', help='Number of epochs to train for', type=int, default=EPOCHS)\n",
    "    args = parser.parse_args()\n",
    "    main(epochs=args.epochs, lr=args.lr, mb_size=args.mb_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transfer the script we created above to the fileshare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az storage file upload --share-name $FILESHARE_NAME --source cifar10_cnn.py --path $SHARE_DIRECTORY \\\n",
    "--account-name $STORAGE_ACCOUNT_NAME --account-key $storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## Create Azure Batch AI Cluster\n",
    "Before we do anything we need to create the cluster for our jobs to run on. This can take a little bit of time so please be patient while the compute nodes are allocated from the Azure Cloud and the Docker images are pre-loaded on to the compute nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do anything though we must create a workspace within which our cluster, experiments and jobs will be associated to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az batchai workspace create --workspace $WORKSPACE --location $LOCATION --resource-group $GROUP_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "Before executing the command below make sure you replace `<admin_username>` and `<admin_password>` with your desired usename and password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NAME = 'gpupool'\n",
    "!az batchai cluster create --name $CLUSTER_NAME --vm-size STANDARD_NC6 --image UbuntuLTS \\\n",
    "--workspace $WORKSPACE \\\n",
    "--min 1 --max 1 --storage-account-name $STORAGE_ACCOUNT_NAME \\\n",
    "--storage-account-key $storage_account_key \\\n",
    "--afs-name $FILESHARE_NAME --afs-mount-path azurefileshare \\\n",
    "--user-name <admin_username> --password <admin_password>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pool is created we can confirm everything by running the commands below. Wait until the State reads steady and the number of nodes in the idle state is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az batchai cluster show -n $CLUSTER_NAME --workspace $WORKSPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az batchai cluster list -w $WORKSPACE -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## Configure Job\n",
    "As before the dictionary below defines the job we will execute. The job specification below tells Batch AI to run the Docker image we defined earlier (`tensorflow/tensorflow:1.8.0-gpu-py3`) and to mount `$AZ_BATCHAI_MOUNT_ROOT/azurefileshare/cnn_example` to the location defined by the environment variable `AZ_BATCHAI_INPUT_SCRIPT`. You will notice that the last word in `AZ_BATCHAI_INPUT_SCRIPT` matches the id we gave to our `inputDirectory`. The Docker image The command simply executes the `cifar10_cnn.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOBNAME=\"tf-training-job\"\n",
    "job = {\n",
    "    \"$schema\": \"https://raw.githubusercontent.com/Azure/BatchAI/master/schemas/2018-05-01/job.json\",\n",
    "    \"properties\": {\n",
    "        \"nodeCount\": 1,\n",
    "        \"customToolkitSettings\": {\n",
    "            \"commandLine\": \"python $AZ_BATCHAI_INPUT_SCRIPT/cifar10_cnn.py\"\n",
    "        },\n",
    "        \"stdOutErrPathPrefix\": os.path.join('$AZ_BATCHAI_MOUNT_ROOT', 'azurefileshare'),\n",
    "        \"inputDirectories\": [{\n",
    "            \"id\": \"SCRIPT\",\n",
    "            \"path\": os.path.join('$AZ_BATCHAI_MOUNT_ROOT', 'azurefileshare', SHARE_DIRECTORY)\n",
    "        }],\n",
    "        \"containerSettings\": {\n",
    "            \"imageSourceRegistry\": {\n",
    "                \"image\": IMAGE_NAME\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file(job,'job.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "## Submit Job\n",
    "Before submitting our job we will create an experiments we can associate the jobs with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az batchai experiment create -n $EXPERIMENT --workspace $WORKSPACE -g $GROUP_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed everything is working we can execute our job using the command below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az batchai job create --experiment $EXPERIMENT --workspace $WORKSPACE --name $JOBNAME \\\n",
    "--cluster $CLUSTER_NAME --config job.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status of the job with the command below. The job should take around 10-15 minutes to run. If it ran without any errors and it has finished the state will read `succeeded` and the exit code will be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az batchai job list -e $EXPERIMENT -w $WORKSPACE -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the command below to list links to the stdout and stderr log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az batchai job file list -e $EXPERIMENT -w $WORKSPACE --job $JOBNAME --output-directory-id stdouterr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can stream any of the text files that are on the node while our job is executing. Interrupt the output by interrupting the kernel (the stop button on the toolbar above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az batchai job file stream -w $WORKSPACE -e $EXPERIMENT --job $JOBNAME --output-directory-id stdouterr --file-name stdout.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az batchai job file stream -w $WORKSPACE -e $EXPERIMENT --job $JOBNAME --output-directory-id stdouterr --file-name stderr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "## Delete job and delete cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az batchai job delete -w $WORKSPACE -e $EXPERIMENT --name $JOBNAME -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az batchai cluster delete -w $WORKSPACE --name $CLUSTER_NAME -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section10'></a>\n",
    "## Delete Azure resources\n",
    "Once you have deleted the pool all that remains is the storage account.\n",
    "\n",
    "Note that you do not need to delete your storage accounts.\n",
    "- Storage costs include data stored in blobs and transactions. For the examples in these notebooks, the cost will be very small.\n",
    "\n",
    "However, if you wish to delete your accounts, you can do so by deleting the resource group containing the accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az group delete --name $GROUP_NAME -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's remove the previously configured default settings for the location and resource group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az configure --defaults group=''\n",
    "!az configure --defaults location=''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
